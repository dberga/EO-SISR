{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SingleImageSR use case\n",
    "\n",
    "The Single Image Super Resolution (SISR) use case is build to compare the image quality between different SiSR solutions. A SiSR algorithm inputs one frame and outputs an image with greater resolution.\n",
    "These are the methods that are being compared in the use case:\n",
    "\n",
    "1. Fast Super-Resolution Convolutional Neural Network (FSRCNN) [Ledig et al., 2016]\n",
    "2. Single Image Super-Resolution Generative Adversarial Networks (SRGAN) [Dong et al., 2016]\n",
    "3. Multi-scale Residual Network (MSRN) [Li et al., 2018]\n",
    "4. Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) [Wang et al., 2018]\n",
    "5. Content Adaptive Resampler (CAR) [Sun & Chen, 2019]\n",
    "6. Local Implicit Image Function (LIIF) [Chen et al., 2021]\n",
    "\n",
    "A use case in IQF usally involves wrapping a training within mlflow framework. In this case we estimate quality on the solutions offered by the different Dataset Modifiers which are the SISR algorithms. Similarity metrics against the Ground Truth are then compared, as well as predicted Quality Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SiSR Execution settings\n",
    "plot_sne = False                         # t-SNE plot? (requires a bit of RAM)\n",
    "plot_visual_comp = True                  # visual comparison?\n",
    "plot_metrics_comp = True                 # metrics comparison?\n",
    "savefig = False                          # save fig or show in notebook\n",
    "use_fake_modifiers = False               # read existing sr output data files instead of modifying?\n",
    "use_existing_metrics = True              # read existing metrics output data files instead of processing them?\n",
    "compute_similarity_metrics = True        # compute these? \n",
    "compute_noise_metrics = True             # compute these?\n",
    "compute_sharpness_metrics = True         # compute these?\n",
    "compute_regressor_quality_metrics = True # compute these?\n",
    "settings_lr_blur = True                  # blur right before modification?\n",
    "settings_resize_preprocess = True        # resize right before modification?\n",
    "settings_resize_postprocess = False      # resize right after modification?\n",
    "settings_zoom = 3                        # scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_ext autoreload\n",
    "#autoreload 2\n",
    "import os\n",
    "import shutil\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pdb import set_trace as debug # debugging\n",
    "\n",
    "## update iquaflow with \"pip3 install git+https://ACCESSTOKEN@github.com/satellogic/iquaflow.git\"\n",
    "from iquaflow.datasets import DSWrapper\n",
    "from iquaflow.experiments import ExperimentInfo, ExperimentSetup\n",
    "from iquaflow.experiments.task_execution import PythonScriptTaskExecution\n",
    "from iquaflow.metrics import SharpnessMetric, SNRMetric\n",
    "from iquaflow.quality_metrics import ScoreMetrics, RERMetrics, SNRMetrics, GaussianBlurMetrics, NoiseSharpnessMetrics, GSDMetrics\n",
    "\n",
    "from custom_modifiers import DSModifierLR, DSModifierFake, DSModifierMSRN, DSModifierFSRCNN,  DSModifierLIIF, DSModifierESRGAN, DSModifierCAR, DSModifierSRGAN\n",
    "from custom_metrics import SimilarityMetrics\n",
    "from visual_comparison import visual_comp, metric_comp, plotSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path of the original(reference) dataset\n",
    "data_path = f\"./Data/test-ds\"\n",
    "images_folder = \"test\"\n",
    "images_path = os.path.join(data_path, images_folder)\n",
    "database_name = os.path.basename(data_path)\n",
    "data_root = os.path.dirname(data_path)\n",
    "\n",
    "#Output\n",
    "plots_folder = \"plots/\"+experiment_name+\"/\"\n",
    "comparison_folder = \"comparison/\"+experiment_name+\"/\"\n",
    "results_folder = \"results/\"+experiment_name+\"/\"\n",
    "\n",
    "#DS wrapper is the class that encapsulate a dataset\n",
    "ds_wrapper = DSWrapper(data_path=data_path)\n",
    "\n",
    "#Define name of IQF experiment\n",
    "experiment_name = \"SiSR\"\n",
    "experiment_name += f\"_{database_name}\"\n",
    "experiment_name += f\"_blur{settings_lr_blur}\"+f\"x{settings_zoom}\"\n",
    "experiment_name += f\"_pre{settings_resize_preprocess}\"+f\"x{settings_zoom}\"\n",
    "experiment_name += f\"_post{settings_resize_postprocess}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove previous mlflow records of previous executions of the same experiment\n",
    "try:\n",
    "    # create output dirs\n",
    "    os.makedirs(plots_folder, exist_ok=True)\n",
    "    os.makedirs(comparison_folder, exist_ok=True)\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    # rm_experiment\n",
    "    mlflow.delete_experiment(ExperimentInfo(f\"{experiment_name}\").experiment_id)\n",
    "    # Clean mlruns and __pycache__ folders\n",
    "    shutil.rmtree(\"mlruns/\",ignore_errors=True)\n",
    "    os.makedirs(\"mlruns/.trash\", exist_ok=True)\n",
    "    shutil.rmtree(f\"{data_path}/.ipynb_checkpoints\",ignore_errors=True)\n",
    "    [shutil.rmtree(x) for x in glob(os.path.join(os.getcwd(), \"**\", '__pycache__'), recursive=True)]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SNE of existing images\n",
    "if plot_sne:\n",
    "    plotSNE(database_name, images_path, (232,232), 6e3, True, savefig, plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of modifications that will be applied to the original dataset:\n",
    "ds_modifiers_list = [\n",
    "    DSModifierLR( params={\n",
    "        'zoom': settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    }),\n",
    "        DSModifierFSRCNN( params={\n",
    "        'config':\"test_scale3.json\",\n",
    "        'model':\"FSRCNN_1to033_x3_blur/best.pth\",\n",
    "        'zoom': settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    } ),\n",
    "    DSModifierSRGAN( params={\n",
    "        #\"arch\": \"srgan_2x2\",\n",
    "        #\"model_path\": \"./models/srgan/weights/PSNR_inria_scale2.pth\",\n",
    "        \"arch\": \"srgan\",\n",
    "        \"model_path\": \"./models/srgan/weights/PSNR_inria_scale4.pth\",\n",
    "        \"gpu\": 0,\n",
    "        \"seed\": 666,\n",
    "        \"zoom\": settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    } ),\n",
    "    DSModifierMSRN( params={\n",
    "        'model':\"MSRN_nonoise/MSRN_1to033/model_epoch_1500.pth\",\n",
    "        'compress': False,\n",
    "        'add_noise': None,\n",
    "        'zoom': settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    } ),\n",
    "    DSModifierESRGAN( params={\n",
    "        'model':\"ESRGAN_1to033_x3_blur/net_g_latest.pth\",\n",
    "        'zoom':settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    } ),\n",
    "    DSModifierCAR( params={\n",
    "        \"SCALE\": 4,\n",
    "        #\"SCALE\": 2,\n",
    "        \"model_dir\": \"./models/car/models\",\n",
    "        \"gpu\": 0,\n",
    "        \"zoom\": settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    } ),\n",
    "    DSModifierLIIF( params={\n",
    "        'config0':\"LIIF_config.json\",\n",
    "        'config1':\"test_liif.yaml\",\n",
    "        'model':\"LIIF_blur/epoch-best.pth\",\n",
    "        'zoom': settings_zoom,\n",
    "        'blur': settings_lr_blur,\n",
    "        'resize_preprocess': settings_resize_preprocess,\n",
    "        'resize_postprocess': settings_resize_postprocess,\n",
    "    } ),\n",
    "]\n",
    "\n",
    "\n",
    "# adding fake modifier of original images (GT)\n",
    "ds_modifiers_list.append(DSModifierFake(name=\"HR\",images_dir=images_path))\n",
    "\n",
    "# check existing modified images and replace already processed modifiers by DSModifierFake (only read images)\n",
    "if use_fake_modifiers: \n",
    "    ds_modifiers_indexes_dict = {}\n",
    "    for idx,ds_modifier in enumerate(ds_modifiers_list):\n",
    "        ds_modifiers_indexes_dict[ds_modifier._get_name()]=idx\n",
    "    ds_modifiers_found = [name for name in glob(os.path.join(data_root,database_name)+\"#*\")]\n",
    "    for sr_folder in ds_modifiers_found:\n",
    "        sr_name = os.path.basename(sr_folder).replace(database_name+\"#\",\"\")\n",
    "        sr_dir=os.path.join(sr_folder,images_folder)\n",
    "        if len(os.listdir(sr_dir)) == len(os.listdir(images_path)) and sr_name in list(ds_modifiers_indexes_dict.keys()):\n",
    "            index_modifier = ds_modifiers_indexes_dict[sr_name]\n",
    "            ds_modifiers_list[index_modifier]=DSModifierFake(name=sr_name,images_dir = sr_dir,params = {\"modifier\": sr_name})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path of the training script\n",
    "python_ml_script_path = 'sr.py'\n",
    "\n",
    "# Task execution executes the training loop\n",
    "task = PythonScriptTaskExecution( model_script_path = python_ml_script_path )\n",
    "\n",
    "#Experiment definition, pass as arguments all the components defined beforehand\n",
    "experiment = ExperimentSetup(\n",
    "    experiment_name=experiment_name,\n",
    "    task_instance=task,\n",
    "    ref_dsw_train=ds_wrapper,\n",
    "    ds_modifiers_list=ds_modifiers_list,\n",
    "    ref_dsw_val=ds_wrapper,\n",
    "    repetitions=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "The number of runs are all the combinations between repetitions, modifiers list as well as hyper parameter changes.\n",
    "\n",
    "(you can skip this step in demo pre-executed datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Execute the experiment\n",
    "experiment.execute()\n",
    "# ExperimentInfo is used to retrieve all the information of the whole experiment. \n",
    "# It contains built in operations but also it can be used to retrieve raw data for futher analysis\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_visual_comp:\n",
    "    print('Visualizing examples')\n",
    "\n",
    "    lst_folders_mod = [os.path.join(data_path+'#'+ds_modifier._get_name(),images_folder) for ds_modifier in ds_modifiers_list]\n",
    "    lst_labels_mod = [ds_modifier._get_name().replace(\"sisr+\",\"\").split(\"_\")[0] for ds_modifier in ds_modifiers_list] # authomatic readout from folders\n",
    "\n",
    "    visual_comp(lst_folders_mod, lst_labels_mod, savefig, comparison_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "ExperimentInfo is used to retrieve all the information of the whole experiment. \n",
    "It contains built in operations but also it can be used to retrieve raw data for futher analysis. Its instance can also be used to apply metrics per run. Some custum metrics are presented. They where build by inheriting Metric from iq_tool_box.metrics.\n",
    "\n",
    "(you can skip this step in demo pre-executed datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "similarity_metrics = ['ssim','psnr','swd','fid', 'ms_ssim','haarpsi','gmsd','mdsi']\n",
    "noise_metrics = ['snr_median','snr_mean']\n",
    "sharpness_metrics = ['RER', 'MTF', 'FWHM']\n",
    "regressor_quality_metrics = ['sigma','snr','rer','sharpness','scale','score']\n",
    "all_metrics = similarity_metrics+noise_metrics+sharpness_metrics+regressor_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Calculating similarity metrics...'+\",\".join(similarity_metrics))\n",
    "path_similarity_metrics = f'./{results_folder}similarity_metrics.csv'\n",
    "if use_existing_metrics and os.path.exists(path_similarity_metrics):\n",
    "    df = pd.read_csv(path_similarity_metrics)\n",
    "elif compute_similarity_metrics:\n",
    "    win = 28\n",
    "    _ = experiment_info.apply_metric_per_run(\n",
    "        SimilarityMetrics(\n",
    "            experiment_info,\n",
    "            n_jobs               = 4,\n",
    "            ext                  = 'tif',\n",
    "            n_pyramids           = 1,\n",
    "            slice_size           = 7,\n",
    "            n_descriptors        = win*2,\n",
    "            n_repeat_projection  = win,\n",
    "            proj_per_repeat      = 4,\n",
    "            device               = 'cuda:0', #'cpu',\n",
    "            return_by_resolution = False,\n",
    "            pyramid_batchsize    = win,\n",
    "            use_liif_loader      = True,\n",
    "            zoom                 = settings_zoom,\n",
    "            blur                 = settings_lr_blur,\n",
    "            resize_preprocess    = settings_resize_preprocess,\n",
    "        ),\n",
    "        ds_wrapper.json_annotations,\n",
    "    )\n",
    "    df = experiment_info.get_df(\n",
    "        ds_params=[\"modifier\"],\n",
    "        metrics=similarity_metrics,\n",
    "        dropna=False\n",
    "    )\n",
    "    df.to_csv(path_similarity_metrics)\n",
    "else:\n",
    "    df = pd.DataFrame(0, index=[0], columns=['ds_modifier']+similarity_metrics); # empty df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_metrics_comp:\n",
    "    metric_comp(df,similarity_metrics,savefig,plots_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise and Sharpness (Blind) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating Noise Metrics...'+\",\".join(noise_metrics))\n",
    "path_noise_metrics = f'./{results_folder}noise_metrics.csv'\n",
    "if use_existing_metrics and os.path.exists(path_noise_metrics):\n",
    "    df = pd.read_csv(path_noise_metrics)\n",
    "elif compute_noise_metrics:\n",
    "    _ = experiment_info.apply_metric_per_run(\n",
    "         SNRMetric(\n",
    "             experiment_info,\n",
    "             ext=\"tif\",\n",
    "             method=\"HB\",\n",
    "             # patch_size=30, #patch_sizes=[30]\n",
    "             #confidence_limit=50.0,\n",
    "             #n_jobs=15\n",
    "         ),\n",
    "         ds_wrapper.json_annotations,\n",
    "     )\n",
    "    df = experiment_info.get_df(\n",
    "        ds_params=[\"modifier\"],\n",
    "        metrics=noise_metrics,\n",
    "        dropna=False\n",
    "    )\n",
    "    df.to_csv(path_noise_metrics)\n",
    "else:\n",
    "    df = pd.DataFrame(0, index=[0], columns=['ds_modifier']+noise_metrics); # empty df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_metrics_comp:\n",
    "    metric_comp(df,noise_metrics,savefig,plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating Sharpness Metrics...'+\",\".join(sharpness_metrics))\n",
    "path_sharpness_metrics = f'./{results_folder}sharpness_metrics.csv'\n",
    "# init from input sharpness metrics and replace list to output with directions (horizontal, vertical, other)\n",
    "sharpness_metric = SharpnessMetric(\n",
    "        experiment_info,\n",
    "        stride=16,\n",
    "        ext=\"tif\",\n",
    "        parallel=False,\n",
    "        metrics=sharpness_metrics,\n",
    "        njobs=1\n",
    "    )\n",
    "sharpness_metrics = sharpness_metric.metric_names #after initialization, update to (output) names\n",
    "if use_existing_metrics and os.path.exists(path_sharpness_metrics):\n",
    "    df = pd.read_csv(path_sharpness_metrics)\n",
    "elif compute_sharpness_metrics:\n",
    "    _ = experiment_info.apply_metric_per_run(\n",
    "        sharpness_metric,\n",
    "        ds_wrapper.json_annotations,\n",
    "    )\n",
    "    df = experiment_info.get_df(\n",
    "        ds_params=[\"modifier\"],\n",
    "        metrics=sharpness_metrics,\n",
    "        dropna=False\n",
    "    )\n",
    "    df.to_csv(path_sharpness_metrics)\n",
    "else:\n",
    "    df = pd.DataFrame(0, index=[0], columns=['ds_modifier']+sharpness_metrics); # empty df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_metrics_comp:\n",
    "    metric_comp(df,sharpness_metrics,savefig,plots_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating Regressor Quality Metrics...'+\",\".join(regressor_quality_metrics)) #default configurations\n",
    "path_regressor_quality_metrics = f'./{results_folder}regressor_quality_metrics.csv'\n",
    "if use_existing_metrics and os.path.exists(path_regressor_quality_metrics):\n",
    "    df = pd.read_csv(path_regressor_quality_metrics)\n",
    "elif compute_regressor_quality_metrics:\n",
    "    _ = experiment_info.apply_metric_per_run(ScoreMetrics(), ds_wrapper.json_annotations)\n",
    "    _ = experiment_info.apply_metric_per_run(RERMetrics(), ds_wrapper.json_annotations)\n",
    "    _ = experiment_info.apply_metric_per_run(SNRMetrics(), ds_wrapper.json_annotations)\n",
    "    _ = experiment_info.apply_metric_per_run(GaussianBlurMetrics(), ds_wrapper.json_annotations)\n",
    "    _ = experiment_info.apply_metric_per_run(NoiseSharpnessMetrics(), ds_wrapper.json_annotations)\n",
    "    _ = experiment_info.apply_metric_per_run(GSDMetrics(), ds_wrapper.json_annotations)\n",
    "    df = experiment_info.get_df(\n",
    "        ds_params=[\"modifier\"],\n",
    "        metrics=regressor_quality_metrics,\n",
    "        dropna=False\n",
    "    )\n",
    "    df.to_csv(path_regressor_quality_metrics)\n",
    "else:\n",
    "    df = pd.DataFrame(0, index=[0], columns=['ds_modifier']+regressor_quality_metrics); # empty df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_metrics_comp:\n",
    "    metric_comp(df,regressor_quality_metrics,savefig,plots_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Comparing all Metrics...'+\",\".join(all_metrics))\n",
    "path_all_metrics = f'./{results_folder}all_metrics.csv'\n",
    "if use_existing_metrics and os.path.exists(path_all_metrics):\n",
    "    df = pd.read(path_all_metrics)\n",
    "elif compute_similarity_metrics and compute_noise_metrics and compute_sharpness_metrics and compute_regressor_quality_metrics:\n",
    "    df = experiment_info.get_df(\n",
    "        ds_params=[\"modifier\"],\n",
    "        metrics=all_metrics,\n",
    "        dropna=False\n",
    "    )\n",
    "    df.to_csv(path_all_metrics)\n",
    "else:\n",
    "    df = pd.concat[df_results]\n",
    "    # df = pd.DataFrame(0, index=[0], columns=['ds_modifier']+all_metrics); # empty df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_metrics_comp:\n",
    "    metric_comp(df,all_metrics,savefig,plots_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
